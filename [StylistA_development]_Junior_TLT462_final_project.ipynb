{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[StylistA development] Junior TLT462 final project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rmajunior/rom317/blob/master/%5BStylistA_development%5D_Junior_TLT462_final_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOpriuaclHQx"
      },
      "source": [
        "# 1. Write-up\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_E_gRvFokY4J"
      },
      "source": [
        "\n",
        "<center> **StylistA: a Deep Learning Development Approach** </center>\n",
        "<br>\n",
        "<center> *Robson Martins de Araujo Junior* </center>\n",
        "<br>\n",
        "<center>   TLT 462 â€“ Dr. Scott Garrigan </center>\n",
        "<br>\n",
        " <center> Lehigh University College of Education </center>\n",
        "\n",
        "  \n",
        "  ## OVERVIEW\n",
        "\n",
        "Unlike existing grammar-correction driven applications such as Grammarly or Microsoft Officeâ€™s colored wavy underlines; my agent will focus on patterns and trends of textual samples. By training StylistA to understand patterns such as amount of words in a sentence, amount of (and type of) punctuation used, exaggerated use of certain parts of the speech (i.e., adjectives, adverbs, interjections), it will possible to output variations in stylistics (contrasted to academic manuscript exemplars), and assist students to improve their writing style by visually highlighting areas of the text that need to be improved.\n",
        "\n",
        "Selecting the most suitable model for a Machine Learning (ML) project is crucial. Since StylistA input is natural language, and its complexity layers, a more sophisticated method would be required to accomplish its goals of learning from a students' writing samples and being able to output a diagnostics report regarding its stylistics. Thus, StylistA will be run by a deep learning (DL) model; DL is a branch of ML where the model attempts to learn autonomously from input data received.\n",
        "\n",
        "The choice for not automatically correcting oneâ€™s textual input is intentional. StylistA is meant to be an education tool, to support students, teachers, and whoever seeks self-improvement and understanding of their own writing practices. If you are looking for an automatic, and oftentimes mindless, self-correction tool, you can stop reading this paper here. \n",
        "\n",
        "## METHOD\n",
        "\n",
        "I decided to conduct the  *[Plan B: TensorFlow (TF) and Keras](https://drive.google.com/open?id=1-v7RltnlpDyiyObd5d85aYfOuDxO1gks)* of my roadmap using Google Colaboratory (colab), a cloud programming notebook. With colab, data are easily structured, feed, analyzed, fit into models, optimized, and deployerd using TF and keras.  Recurrent Neural Networks (RNNs), a deep learning model, are known for being effective handling sequential data, ordered numbers, characters, and words. In addition, after the model is trained with a corpus (i.e., textual sample data files). Because it is a deep learning model, is able to create unique textual pieces based on its learning mechanisms within the hidden layers of its neural network.  \n",
        "\n",
        "RNN/LSTM is short for Recurrent Neural Networks that use Long Short Term Memory, this model allows you \"to train character-level language models based on multi-layer LSTM [nodes]\" (Karpathy, 2015). The most effective and reffered model for character handling is the one developed by Stanford Ph.D. student Andrej Karpathy, and shared in his Github ([@karpathy](https://github.com/karpathy/char-rnn)). LSTM addresses RNNs caveats. Hence, it is a potential model to handle StylistA data input.\n",
        "\n",
        " ### Searching for THE model\n",
        "\n",
        "I started by identifying the best model for the project I had proposed. My prospective dataframe will derive from [Oxford Dictionaries API ](developer.oxforddictionaries.com) , because it is a comprehensive and well-known dictionary worldwide. Then I searched for freely available data sets online (e.g., [Kaggle](https://www.kaggle.com/datasets)) using the following keywords: `dictionary`, `language`, `linguistics`. I found some interesting linguistic datasets, but were not within the scope of StylistA's needs (e.g., some were partial datasets, others were informal or comical language). \n",
        "\n",
        " ### Preparing the data\n",
        "\n",
        "The data used will be composed of many application letters I have written since 2016. Attached are the two most recent samples  [2018-2019](https://drive.google.com/file/d/13SvR0YhAbAEBeZmRk78H8O-_rZ3RIUoM/view?usp=sharing) and [2019-2020](https://drive.google.com/file/d/13SvR0YhAbAEBeZmRk78H8O-_rZ3RIUoM/view?usp=sharing), already edited to serve as corpus, saved as .txt files. This data preparing process has been invaluable, for me, because I can identify the linguistic and stylistic patterns as I clean and organize the sentences in the new file format.\n",
        "\n",
        " ### Gathering the necessary self-instruction material\n",
        "\n",
        "\n",
        "\n",
        "## OUTCOMES\n",
        "\n",
        "* **[Recurrent Neural Networks & Long Short Term Memory Network](https://drive.google.com/open?id=1ClPX8LqAC_KCohtUFL2_7IBfPT-pS6rY)**\n",
        "\n",
        "* [RNN - LSTM with Keras and Scikit-learn.ipynb](https://colab.research.google.com/drive/1Qx08C4UJQ_oht2IuMiv_E5yYfunypWTH) - Tensorflow sequential model with one and two neural network layers with Adam optimization.  \n",
        "\n",
        "* [RNN Implementation using NumPy Sequence input, Char-level, Batch training, Python 3](https://colab.research.google.com/drive/1MN4crCPQcMnsUAwVawQXgeLEAhfCKwVG)\n",
        "Source: @JY-Yoon\n",
        "Original code: @karpathy- \n",
        "\n",
        "* [[Debug] This is a batched LSTM forward and backward pass***](https://drive.google.com/open?id=1UGPh_-r3Rj48YMbuBmjsD9H6xwNHFopd) - A draft / debug file to practice the adjustments needed to convert Python 2  into Python 3 codes.\n",
        "\n",
        "**Note:** I could foresee two programming errors from two youtube instructors! We will go through these in the presentation. \n",
        "\n",
        "<center>![levelUp](https://render.bitstrips.com/v2/cpanel/8c15569e-920d-40ca-a005-52d026c9bde5-6e797bc6-7564-4366-8671-8f7b2b27407f-v1.png?transparent=1&palette=1&width=150)</center>\n",
        "\n",
        "---\n",
        "\n",
        "## REFERENCES\n",
        "\n",
        "Karpathy, A. (2015). The Unreasonable Effectiveness of Recurrent Neural Networks\n",
        "http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
        "Published on May 21, 2015\n",
        "\n",
        "## WEBLIOGRAPHY\n",
        "\n",
        "1. What are Recurrent Neural Networks (RNN) and Long Short Term Memory Networks (LSTM) ?\n",
        "The Semicolon\n",
        "https://youtu.be/S0XFd0VMFss\n",
        "Published on Jan 30, 2018\n",
        "\n",
        "2. Recurrent Neural Networks (LSTM / RNN) Implementation with Keras - Python\n",
        "The Semicolon\n",
        "https://youtu.be/iMIWee_PXl8\n",
        "Published on Mar 19, 2018\n",
        "\n",
        "3. [Deep Learning with Python, TensorFlow, and Keras tutorial](https://pythonprogramming.net/introduction-deep-learning-python-tensorflow-keras)\n",
        "[link text](https://)\n",
        "\n",
        "4. Recurrent Neural Networks (RNNs)  - Deep Learning w/ Python, TensorFlow & Keras [[Part 7]](https://youtu.be/BSpXCRTOLJA) .From the Youtube Channel **sentdex** series: [Deep Learning basics with Python, TensorFlow and Keras](https://www.youtube.com/watch?v=wQ8BIBpya2k&list=PLQVvvaa0QuDfhTox0AjmQ6tvTgMBZBEXN)\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNCxeuKalP6g"
      },
      "source": [
        "# 2. The Recurrent Neural Network (RNN) with Long Short Term Memory (LSTM) Model "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OgyF2COlwN5"
      },
      "source": [
        "## 2.1 Setting up the RNN/LSTM model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzwHMUEQl8_g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "20562579-c203-4e07-bd0a-8b92f9d8b339"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8RHrtJiw613"
      },
      "source": [
        "Notice that we are using keras and scikit-learn! ðŸ§"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "siQ1YNshmFbg"
      },
      "source": [
        "## 2.2 Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcA8Zpbpmi_B"
      },
      "source": [
        "---\n",
        "### This is the NON-normalized data  *(Not to be run! Just illustration purposes)*\n",
        "\n",
        "```\n",
        "data = [[[i+j] for i in range(5)] for j in range(100)]\n",
        "target = [(i+5) for i in range(100)]\n",
        "```\n",
        "\n",
        ">Although this is not going to produce any significant outcome, this piece of code has played a major role in my learning process! Thanks to it, I was able to learn **the importance of normalizing the data** before fitting it into the selected machine learning model.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hu_18bswos3o"
      },
      "source": [
        "## 2.2.1 Data NORMALIZATION\n",
        "\n",
        "<center> \n",
        "    \n",
        "#####\"Normalization is the process of organizing data in a database.\" (Microsoft Support, 2017)\n",
        "\n",
        "#####\"When we do not normalize the data, it makes the gradient very high. Thus, making it difficult for the model to learn anything!\"\" (The semicolon, 2018)</center><br>\n",
        "\n",
        "For this ML model practice example, our dataset has 100 rows (vectors) only composed by integers.\n",
        "Therefore, in order to **simplify** this dataframe, we can divide each entry (each row) by 100, which will **evenly reduce** the numbers making this dataframe easier for the algorithym to process.\n",
        "\n",
        "><br> **Note:** It should take place early on, in the data preparation stage."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MJd5nQ8mHuA"
      },
      "source": [
        "## NOW THE REAL DEAL\n",
        "Data = [[[(i+j)/100] for i in range(5)] for j in range(100)]\n",
        "target = [(i+5)/100 for i in range(100)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "809cNlpBncEY"
      },
      "source": [
        ">**Note:** When dealing with text and images datasets, the vector sizes in the previous line of code may change.  \n",
        "                   In this example, 5 is a sufficient number to use; we will call five sequential numbers, and the machine should predict the following number (target), that is, the **outcome** of this algorithm.                \n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgRggY2pxxEH"
      },
      "source": [
        "By typing the name of the variable and runing the line of code, we are requesting to visualize the its value/format/shape.\n",
        "\n",
        "In our case:\n",
        "\n",
        "*   ```data```  >> will return a textual representation of our dataframe.\n",
        "*   ```target``` >> will return the expected outcome, that is, the following number after a sequence of five consecutive numbers.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UObYXIuz1BfG"
      },
      "source": [
        "## 2.2.2 Let's convert these to numpy arrays"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qywd5vz51LdO"
      },
      "source": [
        "data = np.array(Data, dtype=float)\n",
        "target = np.array(target,dtype=float)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfGp9Bdz1QWP"
      },
      "source": [
        ">### We will be using these shapes as parameters when we implement LSTMs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdZ0dxUV1U3W",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e86b9a8a-c299-4229-be82-0b84dff29f77"
      },
      "source": [
        "data.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100, 5, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTC-L84C1Wdm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "005ebebc-8d7c-4a5c-b4dd-959ae96bffd8"
      },
      "source": [
        "target.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLb6s5hM1ob2"
      },
      "source": [
        "## 2.2.3 Splitting the data into Test and Train\n",
        "This is the last step of data preparation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nEkqvcn1wqe"
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(data, target, test_size = 0.2, random_state = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCOc_0AAJ4QT"
      },
      "source": [
        "---\n",
        "## 2.3 RNN model\n",
        "\n",
        "Implementing LSTM, and watching it learn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOCK8VDfJ2-M"
      },
      "source": [
        "model=Sequential()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvDbA5WlPLXb"
      },
      "source": [
        "The `Sequential` model is a linear stack of layers.\n",
        "\n",
        "![Stacked LSTM for sequence classification](https://keras.io/img/regular_stacked_lstm.png)\n",
        "\n",
        "Source: https://keras.io/getting-started/sequential-model-guide/\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yzWMHuxOb8E",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "be19f60a-faa4-4be3-add5-8381175e702a"
      },
      "source": [
        "model.add(LSTM((1), batch_input_shape=(None,5,1), return_sequences=False))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdbbTM6wLD1z"
      },
      "source": [
        "To add LSTM later, we need to specify some parameters. \n",
        "\n",
        "1.   **Input size** ```LSTM(1)```\n",
        ">This value is 1, because we want ONE output.\n",
        "2.   **Batch input shape** ```batch_input_shape=(a,b,c)```\n",
        ">Describes the output of our data. As follows: number of inputs (a), length of input sequences (b), and length of each vector.\n",
        "\n",
        ">>\"According to [Keras documentation on Sequential models](https://keras.io/getting-started/sequential-model-guide/), this `batch` argument is useful for ***stateful*** RNNs. A *stateful recurrent model* is one for which the internal states (memories) obtained after processing a batch of samples are reused as initial states for the samples of the next batch. This allows to <u>process longer sequences</u> while keeping computational complexity manageable.\" (keras.io)\n",
        "\n",
        ">**Note:** If we do not know the number of inputs in our data, we can replace the value with **None**\n",
        "\n",
        "3. **return_sequences=_boolean_** \n",
        "> Determines whether we want output to be printed after every node. If set to **true**, it would return output after every node, which would cause an error in our code. Setting it to **False** will return only one output after the end of the last node.\n",
        "\n",
        ">**Note:** These variables are <u>case sensitive</u>!  Therefore, they must be entered as **N**one; **T**rue or **F**alse."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAGEKa6UOgBb"
      },
      "source": [
        "model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uw7yRFI3OotL"
      },
      "source": [
        "**Compile** is where we set up the learning process. It usually takes three arguments: **loss**, **optimizer**, and **metrics**.\n",
        "\n",
        "For now, the parameters are the same as used in the tutorials I followed. Changing and experimenting will be a delightful summer experience! ðŸ˜Ž\n",
        "\n",
        "The aforementioned Keras documentation is loaded with many examples and brief explanations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnoz0ca2Oo_C",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "52f04122-11d5-40fd-f4b4-00577d4265b5"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_1 (LSTM)                (None, 1)                 12        \n",
            "=================================================================\n",
            "Total params: 12\n",
            "Trainable params: 12\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83rptDYbOpjC"
      },
      "source": [
        "**model.summary()** allows up to verify the <u>shape</u> and the <u>number of parameters</u>\n",
        "\n",
        ">**Note:** This is a simple instance of RNN/LSTM with only 1 layer, that was meant to execute a rather simple prediction request. However, this material and its video tutorial have been great assets for improving my skillset to develop StylistA. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MB6DNfvZn5a"
      },
      "source": [
        "## 2.4 Fitting the model on the training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQdbpSFXZxbJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1805
        },
        "outputId": "12319f45-49a5-4b87-f1f5-75c42ba93012"
      },
      "source": [
        "history = model.fit(x_train, y_train, epochs=100, validation_data=(x_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 80 samples, validate on 20 samples\n",
            "Epoch 1/50\n",
            "80/80 [==============================] - 1s 14ms/step - loss: 0.3314 - acc: 0.0000e+00 - val_loss: 0.2436 - val_acc: 0.0000e+00\n",
            "Epoch 2/50\n",
            "80/80 [==============================] - 0s 249us/step - loss: 0.3290 - acc: 0.0000e+00 - val_loss: 0.2409 - val_acc: 0.0000e+00\n",
            "Epoch 3/50\n",
            "80/80 [==============================] - 0s 224us/step - loss: 0.3264 - acc: 0.0000e+00 - val_loss: 0.2381 - val_acc: 0.0000e+00\n",
            "Epoch 4/50\n",
            "80/80 [==============================] - 0s 225us/step - loss: 0.3238 - acc: 0.0000e+00 - val_loss: 0.2353 - val_acc: 0.0000e+00\n",
            "Epoch 5/50\n",
            "80/80 [==============================] - 0s 213us/step - loss: 0.3212 - acc: 0.0000e+00 - val_loss: 0.2325 - val_acc: 0.0000e+00\n",
            "Epoch 6/50\n",
            "80/80 [==============================] - 0s 208us/step - loss: 0.3185 - acc: 0.0000e+00 - val_loss: 0.2297 - val_acc: 0.0000e+00\n",
            "Epoch 7/50\n",
            "80/80 [==============================] - 0s 226us/step - loss: 0.3159 - acc: 0.0000e+00 - val_loss: 0.2268 - val_acc: 0.0000e+00\n",
            "Epoch 8/50\n",
            "80/80 [==============================] - 0s 209us/step - loss: 0.3132 - acc: 0.0000e+00 - val_loss: 0.2240 - val_acc: 0.0000e+00\n",
            "Epoch 9/50\n",
            "80/80 [==============================] - 0s 211us/step - loss: 0.3105 - acc: 0.0000e+00 - val_loss: 0.2211 - val_acc: 0.0000e+00\n",
            "Epoch 10/50\n",
            "80/80 [==============================] - 0s 185us/step - loss: 0.3079 - acc: 0.0000e+00 - val_loss: 0.2183 - val_acc: 0.0000e+00\n",
            "Epoch 11/50\n",
            "80/80 [==============================] - 0s 205us/step - loss: 0.3053 - acc: 0.0000e+00 - val_loss: 0.2156 - val_acc: 0.0000e+00\n",
            "Epoch 12/50\n",
            "80/80 [==============================] - 0s 213us/step - loss: 0.3029 - acc: 0.0000e+00 - val_loss: 0.2132 - val_acc: 0.0000e+00\n",
            "Epoch 13/50\n",
            "80/80 [==============================] - 0s 233us/step - loss: 0.3006 - acc: 0.0000e+00 - val_loss: 0.2108 - val_acc: 0.0000e+00\n",
            "Epoch 14/50\n",
            "80/80 [==============================] - 0s 194us/step - loss: 0.2985 - acc: 0.0000e+00 - val_loss: 0.2085 - val_acc: 0.0000e+00\n",
            "Epoch 15/50\n",
            "80/80 [==============================] - 0s 203us/step - loss: 0.2966 - acc: 0.0000e+00 - val_loss: 0.2063 - val_acc: 0.0000e+00\n",
            "Epoch 16/50\n",
            "80/80 [==============================] - 0s 214us/step - loss: 0.2948 - acc: 0.0000e+00 - val_loss: 0.2042 - val_acc: 0.0000e+00\n",
            "Epoch 17/50\n",
            "80/80 [==============================] - 0s 217us/step - loss: 0.2930 - acc: 0.0000e+00 - val_loss: 0.2024 - val_acc: 0.0000e+00\n",
            "Epoch 18/50\n",
            "80/80 [==============================] - 0s 206us/step - loss: 0.2914 - acc: 0.0000e+00 - val_loss: 0.2005 - val_acc: 0.0000e+00\n",
            "Epoch 19/50\n",
            "80/80 [==============================] - 0s 243us/step - loss: 0.2897 - acc: 0.0000e+00 - val_loss: 0.1988 - val_acc: 0.0000e+00\n",
            "Epoch 20/50\n",
            "80/80 [==============================] - 0s 240us/step - loss: 0.2882 - acc: 0.0000e+00 - val_loss: 0.1972 - val_acc: 0.0000e+00\n",
            "Epoch 21/50\n",
            "80/80 [==============================] - 0s 211us/step - loss: 0.2866 - acc: 0.0000e+00 - val_loss: 0.1957 - val_acc: 0.0000e+00\n",
            "Epoch 22/50\n",
            "80/80 [==============================] - 0s 207us/step - loss: 0.2850 - acc: 0.0000e+00 - val_loss: 0.1944 - val_acc: 0.0000e+00\n",
            "Epoch 23/50\n",
            "80/80 [==============================] - 0s 203us/step - loss: 0.2835 - acc: 0.0000e+00 - val_loss: 0.1931 - val_acc: 0.0000e+00\n",
            "Epoch 24/50\n",
            "80/80 [==============================] - 0s 247us/step - loss: 0.2820 - acc: 0.0000e+00 - val_loss: 0.1918 - val_acc: 0.0000e+00\n",
            "Epoch 25/50\n",
            "80/80 [==============================] - 0s 242us/step - loss: 0.2806 - acc: 0.0000e+00 - val_loss: 0.1906 - val_acc: 0.0000e+00\n",
            "Epoch 26/50\n",
            "80/80 [==============================] - 0s 292us/step - loss: 0.2791 - acc: 0.0000e+00 - val_loss: 0.1895 - val_acc: 0.0000e+00\n",
            "Epoch 27/50\n",
            "80/80 [==============================] - 0s 279us/step - loss: 0.2776 - acc: 0.0000e+00 - val_loss: 0.1884 - val_acc: 0.0000e+00\n",
            "Epoch 28/50\n",
            "80/80 [==============================] - 0s 236us/step - loss: 0.2762 - acc: 0.0000e+00 - val_loss: 0.1874 - val_acc: 0.0000e+00\n",
            "Epoch 29/50\n",
            "80/80 [==============================] - 0s 210us/step - loss: 0.2747 - acc: 0.0000e+00 - val_loss: 0.1863 - val_acc: 0.0000e+00\n",
            "Epoch 30/50\n",
            "80/80 [==============================] - 0s 234us/step - loss: 0.2733 - acc: 0.0000e+00 - val_loss: 0.1852 - val_acc: 0.0000e+00\n",
            "Epoch 31/50\n",
            "80/80 [==============================] - 0s 224us/step - loss: 0.2720 - acc: 0.0000e+00 - val_loss: 0.1842 - val_acc: 0.0000e+00\n",
            "Epoch 32/50\n",
            "80/80 [==============================] - 0s 219us/step - loss: 0.2706 - acc: 0.0000e+00 - val_loss: 0.1831 - val_acc: 0.0000e+00\n",
            "Epoch 33/50\n",
            "80/80 [==============================] - 0s 221us/step - loss: 0.2693 - acc: 0.0000e+00 - val_loss: 0.1820 - val_acc: 0.0000e+00\n",
            "Epoch 34/50\n",
            "80/80 [==============================] - 0s 209us/step - loss: 0.2680 - acc: 0.0000e+00 - val_loss: 0.1810 - val_acc: 0.0000e+00\n",
            "Epoch 35/50\n",
            "80/80 [==============================] - 0s 219us/step - loss: 0.2667 - acc: 0.0000e+00 - val_loss: 0.1800 - val_acc: 0.0000e+00\n",
            "Epoch 36/50\n",
            "80/80 [==============================] - 0s 314us/step - loss: 0.2654 - acc: 0.0000e+00 - val_loss: 0.1791 - val_acc: 0.0000e+00\n",
            "Epoch 37/50\n",
            "80/80 [==============================] - 0s 219us/step - loss: 0.2641 - acc: 0.0000e+00 - val_loss: 0.1782 - val_acc: 0.0000e+00\n",
            "Epoch 38/50\n",
            "80/80 [==============================] - 0s 246us/step - loss: 0.2628 - acc: 0.0000e+00 - val_loss: 0.1773 - val_acc: 0.0000e+00\n",
            "Epoch 39/50\n",
            "80/80 [==============================] - 0s 225us/step - loss: 0.2616 - acc: 0.0000e+00 - val_loss: 0.1764 - val_acc: 0.0000e+00\n",
            "Epoch 40/50\n",
            "80/80 [==============================] - 0s 231us/step - loss: 0.2603 - acc: 0.0000e+00 - val_loss: 0.1755 - val_acc: 0.0000e+00\n",
            "Epoch 41/50\n",
            "80/80 [==============================] - 0s 218us/step - loss: 0.2591 - acc: 0.0000e+00 - val_loss: 0.1746 - val_acc: 0.0000e+00\n",
            "Epoch 42/50\n",
            "80/80 [==============================] - 0s 201us/step - loss: 0.2578 - acc: 0.0000e+00 - val_loss: 0.1737 - val_acc: 0.0000e+00\n",
            "Epoch 43/50\n",
            "80/80 [==============================] - 0s 263us/step - loss: 0.2566 - acc: 0.0000e+00 - val_loss: 0.1728 - val_acc: 0.0000e+00\n",
            "Epoch 44/50\n",
            "80/80 [==============================] - 0s 237us/step - loss: 0.2554 - acc: 0.0000e+00 - val_loss: 0.1719 - val_acc: 0.0000e+00\n",
            "Epoch 45/50\n",
            "80/80 [==============================] - 0s 205us/step - loss: 0.2542 - acc: 0.0000e+00 - val_loss: 0.1710 - val_acc: 0.0000e+00\n",
            "Epoch 46/50\n",
            "80/80 [==============================] - 0s 230us/step - loss: 0.2530 - acc: 0.0000e+00 - val_loss: 0.1701 - val_acc: 0.0000e+00\n",
            "Epoch 47/50\n",
            "80/80 [==============================] - 0s 271us/step - loss: 0.2518 - acc: 0.0000e+00 - val_loss: 0.1692 - val_acc: 0.0000e+00\n",
            "Epoch 48/50\n",
            "80/80 [==============================] - 0s 266us/step - loss: 0.2506 - acc: 0.0000e+00 - val_loss: 0.1682 - val_acc: 0.0000e+00\n",
            "Epoch 49/50\n",
            "80/80 [==============================] - 0s 220us/step - loss: 0.2494 - acc: 0.0000e+00 - val_loss: 0.1673 - val_acc: 0.0000e+00\n",
            "Epoch 50/50\n",
            "80/80 [==============================] - 0s 229us/step - loss: 0.2483 - acc: 0.0000e+00 - val_loss: 0.1664 - val_acc: 0.0000e+00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddvCD4yAZ6LC"
      },
      "source": [
        "###![Hmmmm](https://render.bitstrips.com/v2/cpanel/e7923ac0-cf49-45c8-8414-0904956037cb-6e797bc6-7564-4366-8671-8f7b2b27407f-v1.png?transparent=1&palette=1&width=100) In order to make this amazing process more accessible to a broader audience, let us plot this output!<br><br>\n",
        "Then, we will be able to judge whether the model is learning, and to adjust parameters to obtain the most optimal results. <br><br>\n",
        ">**REMINDER:** this iteration had **epochs=50**, which means the machine went through the **x** and **y** data.  We usually start small to evaluate the learning rate without spending much time and computing process. \n",
        "\n",
        "\n",
        "After plotting and interpreting the visual representation of the outcome, we are encouraged to <u>increase the epochs to find</u> its **most optimal** value.  (see Recap section to read more about finding the best parameters for our model)\n",
        "<center>\n",
        "    \n",
        "![alt text](https://render.bitstrips.com/v2/cpanel/55f5faaa-bbee-4e30-bb67-094748197f8f-6e797bc6-7564-4366-8671-8f7b2b27407f-v1.png?transparent=1&palette=1&width=150)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7POjVsfnZXgB"
      },
      "source": [
        "## 2.5 Checking model efficiency (i.e., is it learning well?)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gceFZLfAdCRZ"
      },
      "source": [
        "results = model.predict(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fsCi9gfc4EK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "c8e5d73f-e886-470e-8456-7037e12dd27d"
      },
      "source": [
        "plt.scatter(range(20), results, c='r')\n",
        "plt.scatter(range(20), y_test, c='b')\n",
        "plt.show()\n",
        "\n",
        "#Where <c='r'> means color = red\n",
        "#['r', 'g', 'b', 'k', 'y', 'm', 'c'] stand for red, green, blue, black, etc."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n8HXnRPW7AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkg7JFp_eAXZ"
      },
      "source": [
        "### ![thumbsUp](https://render.bitstrips.com/v2/cpanel/12f3a5e1-c85f-49cd-94f4-6dce9cb18ef1-6e797bc6-7564-4366-8671-8f7b2b27407f-v1.png?transparent=1&palette=1&width=100) The model is learning well! <br><br>\n",
        "\n",
        " <font color=\"blue\">**Blue dots**</font> represent 20 prediction attempts the model made. <font color=\"red\">**Red dots**</font> are the actual prediction output. Thus, **the closer** red  dots are to blue dots, **the more accurate** the prediction has been. \n",
        " \n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RO3cczVohwk6"
      },
      "source": [
        "## 2.6 Tunning the model\n",
        "\n",
        "### Remember we used only 50 epochs in our first run? Let us analyze how optimal it was running 50 times."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MB0fhv4RiTrx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "b61213f4-0bbd-4bd7-b0df-4d98394f205d"
      },
      "source": [
        "plt.plot(history.history['loss'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f043593d828>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGIM9OAAifNY"
      },
      "source": [
        "### The plot above shows the rate of loss (y) over time/epochs (x).  \n",
        "![sliding](https://render.bitstrips.com/v2/cpanel/9e3e23da-040b-4664-a782-65f9a743b3f4-6e797bc6-7564-4366-8671-8f7b2b27407f-v1.png?transparent=1&palette=1&width=150)\n",
        "\n",
        "By the time the model ran the last iteration (i.e., epoch number 50), it had a decreasing tendency an angle greater than 0  and smaller than 90.\n",
        "\n",
        "It means we can use a larger number of epochs to improve our learning rate!  As long as the loss is decreasing, we can continue increasing epochs. \n",
        "\n",
        "Once the plot shows a more stagnate tendency (i.e., curve is becoming a parallel line to the x axis), it means the machine will not learn much more. In addition, raising the epoch value is not computationally cost-efficient from this point forward.\n",
        "\n",
        "\n",
        "It is time to locate the most optimal epoch value in the loss plot. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxsSvfsn8cvD"
      },
      "source": [
        "## 2.6.1 Here is an example from another iteration I ran:\n",
        "\n",
        "Random sequence=20; Epochs = 300\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "x_train, x_test, y_train, y_test = train_test_split(data, target, test_size = 0.2, random_state = 20)\n",
        "\n",
        "history = model.fit(x_train, y_train, epochs=300, validation_data=(x_test, y_test))\n",
        "```\n",
        "\n",
        "**The learning rate was great!** Here is for iterations, epochs 1, 100, 200, and 300, respectively.\n",
        "\n",
        ">Epoch 1/300\n",
        "80/80 [==============================] - 1s 10ms/step - loss: 0.5205 - acc: 0.0000e+00 - val_loss: 0.5417 - val_acc: 0.0000e+00\n",
        "\n",
        ">Epoch 100/300\n",
        "80/80 [==============================] - 0s 237us/step - loss: 0.0651 - acc: 0.0125 - val_loss: 0.0704 - val_acc: 0.0000e+00\n",
        "\n",
        ">Epoch 200/300\n",
        "80/80 [==============================] - 0s 209us/step - loss: 0.0164 - acc: 0.0125 - val_loss: 0.0188 - val_acc: 0.0000e+00\n",
        "\n",
        ">Epoch 300/300\n",
        "80/80 [==============================] - 0s 224us/step - loss: 0.0146 - acc: 0.0125 - val_loss: 0.0164 - val_acc: 0.0000e+00\n",
        "\n",
        "###Notice how the learning rate is great through the first 200 iterations. <font color=\"green\">Î”loss = 0.455â€¬, epoch range 1-200 </font>. However, the same effectiveness does not apply to the last 100. <font color=\"red\"> Î”loss = 0.0018â€¬â€¬, epoch range 201-300.</font> Thus, we can conclude that <u>the optimal number of epochs should be less than 200</u>.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umQTQVkWB0KY"
      },
      "source": [
        "## 2.6.2 In search for the optimal epoch...\n",
        "\n",
        "We can identify the exact moment the learning rate starts to become stagnant if we analyze <u>the loss trajectory</u> from the **learning rate vs. loss increase plot**.\n",
        "\n",
        "![lossplot.jpg](https://i.postimg.cc/hGtXSdVd/lossplot.jpg)\n",
        "\n",
        "### According to the figure above, 150 would be the optimal epoch to be run in this model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FFADRp4HkOn"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# 3. Recap\n",
        "\n",
        "\n",
        "1. Data preparation;\n",
        "2. Normalize it;\n",
        "3. *Train / Test Split* with **scikit-learn**;\n",
        "4. Use **numpy** to check the data structure (shape and number of parameters);\n",
        "5. Fit the data;\n",
        "6. Plot for analysis of effectiveness (learning rate vs. loss increase);\n",
        "7. Increase the number of epochs to find its ideal iteration.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Note:\n",
        "### If the model hasn't learned anything, we didn't normalize it properly... The next section is about the procedures I have taken to ensure we obtain success in setting up this ML project.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ykrkm9zrH2QO"
      },
      "source": [
        "---\n",
        "---\n",
        "\n",
        "## 3.1 TROUBLESHOOTING:\n",
        "\n",
        "*   Reset all runtime processes;\n",
        "\n",
        "*   Clear all outputs;\n",
        "\n",
        "*   Start over;\n",
        "\n",
        "*   Normalize your data at the beginning! Do not Train /Split it without Normalization, otherwise the gradient will be very high (i.e., inflates the variance and error);\n",
        "\n",
        "* You'll notice your loss is still decreasing, and has not reached a stagnation level (i.e., does not go down any further) >>>> Increase the EPOCH number;\n",
        "\n",
        "* Re-run ;\n",
        "\n",
        "* Repeat these last steps until your loss reaches its stable level;\n",
        "\n",
        "* Check the loss plot again IDENTIFY the EXACT number of EPOCH when it stopped plummeting. \n",
        "\n",
        "<CENTER>\n",
        "    \n",
        "![deep breaths](https://render.bitstrips.com/v2/cpanel/f5fed534-1f42-440c-8b23-1ebe8bfda9de-6e797bc6-7564-4366-8671-8f7b2b27407f-v1.png?transparent=1&palette=1&width=150)\n",
        "\n",
        "\n",
        "## That will be your ideal parameter for running the model again from scratch!\n",
        "    \n",
        "![alt text](https://render.bitstrips.com/v2/cpanel/7652499a-75e7-4a53-b4d2-47726289c68a-6e797bc6-7564-4366-8671-8f7b2b27407f-v1.png?transparent=1&palette=1&width=150)\n",
        "\n",
        "</center>\n",
        "\n",
        "---\n",
        "---\n",
        "\n",
        "## FINAL NOTE:\n",
        "### Although it may sound crazy to reset everything and start over, now that you found your IDEAL EPOCH number for the model, you will avoid OVERFITTING !\n",
        "\n",
        "<CENTER>\n",
        "    \n",
        "![deep breaths](https://render.bitstrips.com/v2/cpanel/ea735ccc-d6b0-4ea8-a360-9c21812a2683-6e797bc6-7564-4366-8671-8f7b2b27407f-v1.png?transparent=1&palette=1&width=150)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nn1FRecDJzXe"
      },
      "source": [
        "# 4. Why RNN/LSTM to run StylistA?\n",
        " ![why?](https://render.bitstrips.com/v2/cpanel/f4c91480-fbcf-41d6-8afe-2c9b58d18798-6e797bc6-7564-4366-8671-8f7b2b27407f-v1.png?transparent=1&width=250)\n",
        " \n",
        "## Because...\n",
        "\n",
        "### In *Natural Language*, sentence word order plays an important role in semantics (i.e., the meaning of sentences)!\n",
        "\n",
        "<center>\n",
        "\n",
        "![bitmojr](https://render.bitstrips.com/v2/cpanel/c8c182f8-9573-4964-8a76-cb7e0db89729-6e797bc6-7564-4366-8671-8f7b2b27407f-v1.png?transparent=1&palette=1&width=200)\n",
        "    \n",
        " </center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pS0jNsVHL-7u"
      },
      "source": [
        "## 4.1 Another way to set up a RNN/LSTM with TensorFlow\n",
        "###We are using mnist dataset this time for convenience."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stAnzKAHLmom",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "0b8f71f8-9a2a-4525-c28b-3f2483d1b044"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, LSTM #CuDNNLSTM is an upgraded and faster way, but it NEEDS to be run with a GPU. \n",
        "\n",
        "mnist = tf.keras.datasets.mnist  # mnist is a dataset of 28x28 images of handwritten digits and their labels\n",
        "(x_train2, y_train2),(x_test2, y_test2) = mnist.load_data()  # unpacks images to x_train/x_test and labels to y_train/y_test\n",
        "\n",
        "x_train2 = x_train2/255.0\n",
        "x_test2 = x_test2/255.0\n",
        "\n",
        "print(x_train2.shape)\n",
        "print(x_train2[0].shape)\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(LSTM(128, input_shape=(x_train2.shape[1:]), activation='relu', return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(LSTM(128, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "          \n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dropout(0.2))      \n",
        "          \n",
        "model.add(Dense(10, activation='softmax'))    \n",
        "          \n",
        "opt = tf.keras.optimizers.Adam(lr=1e-3, decay=1e-5)  #decay reduces the learning rate, so that it takes smaller steps instead of be jumping around\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "\n",
        " #mean squared error  MSE is another way of measuring loss\n",
        "\n",
        "model.fit(x_train2, y_train2, epochs=3, validation_data=(x_test2,y_test2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n",
            "(28, 28)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/3\n",
            "60000/60000 [==============================] - 403s 7ms/sample - loss: 0.5939 - acc: 0.8076 - val_loss: 0.1549 - val_acc: 0.9536\n",
            "Epoch 2/3\n",
            "60000/60000 [==============================] - 404s 7ms/sample - loss: 0.1592 - acc: 0.9567 - val_loss: 0.1030 - val_acc: 0.9713\n",
            "Epoch 3/3\n",
            "60000/60000 [==============================] - 403s 7ms/sample - loss: 0.1120 - acc: 0.9699 - val_loss: 0.0723 - val_acc: 0.9800\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fad24a2aa90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGASDGKGVZw1"
      },
      "source": [
        "# 5. Another model that will be very useful for StylistA development\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWhMkWp_wZex"
      },
      "source": [
        "https://colab.research.google.com/drive/1MN4crCPQcMnsUAwVawQXgeLEAhfCKwVG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vszBew-pkLhx"
      },
      "source": [
        "# 6. References (WEBLIOGRAPHY)\n",
        "\n",
        "1. Karpathy, A. (2015). The Unreasonable Effectiveness of Recurrent Neural Networks http://karpathy.github.io/2015/05/21/rnn-effectiveness/ Published on May 21, 2015\n",
        "\n",
        "2. Keras documentation.  https://keras.io/getting-started/sequential-model-guide/#sequence-classification-with-lstm\n",
        "\n",
        "3. Microsoft Support. (2017).**Description of the database normalization basics**. Retrieved May 8, 2019, from https://support.microsoft.com/en-us/help/283878\n",
        " \n",
        "4. Scikit-learn: Machine Learning in Python, Pedregosa et al., JMLR 12, pp. 2825-2830, 2011. http://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html\n",
        " \n",
        "4. The Semicolon. (2018). **Recurrent Neural Networks (LSTM / RNN) Implementation with Keras - Python**  \n",
        "Retrieved April 29, 2019, from https://youtu.be/iMIWee_PXl8\n",
        "\n",
        "5. _________________. (2018). **What are Recurrent Neural Networks (RNN) and Long Short Term Memory Networks (LSTM)?**\n",
        "Retrieved April 29, 2019, from https://youtu.be/iMIWee_PXl8 https://youtu.be/S0XFd0VMFss Published on Jan 30, 2018\n",
        "\n",
        "6. Sentdex. (2018). **Recurrent Neural Networks (RNNs): Deep Learning w/ Python, TensorFlow & Keras [Part 7] .**\n",
        "   Retrieved April 28, 2019, from https://www.youtube.com/watch?v=wQ8BIBpya2k&list=PLQVvvaa0QuDfhTox0AjmQ6tvTgMBZBEXN"
      ]
    }
  ]
}